## Question 1

**Use Amazon Route 53 based geolocation routing policy to restrict distribution of content to only the locations in which you have distribution rights**

Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. You can also use geolocation routing to restrict the distribution of content to only the locations in which you have distribution rights.

**Use georestriction to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution**

You can use georestriction, also known as geo-blocking, to prevent users in specific geographic locations from accessing content that you're distributing through a Amazon CloudFront web distribution. When a user requests your content, Amazon CloudFront typically serves the requested content regardless of where the user is located. If you need to prevent users in specific countries from accessing your content, you can use the CloudFront geo restriction feature to do one of the following: Allow your users to access your content only if they're in one of the countries on a whitelist of approved countries. Prevent your users from accessing your content if they're in one of the countries on a blacklist of banned countries. So this option is also correct.

Amazon Route 53 Routing Policy Overview: ![](https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q38-i1.jpg)

![](https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q38-i1.jpg)

via - [https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html)

### Incorrect options:

**Use Amazon Route 53 based latency-based routing policy to restrict distribution of content to only the locations in which you have distribution rights** - Use latency-based routing when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the lowest latency. To use latency-based routing, you create latency records for your resources in multiple AWS Regions. When Amazon Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Route 53 responds with the value from the selected record, such as the IP address for a web server.

**Use Amazon Route 53 based weighted routing policy to restrict distribution of content to only the locations in which you have distribution rights** - Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of the software.

**Use Amazon Route 53 based failover routing policy to restrict distribution of content to only the locations in which you have distribution rights** - Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy. The primary and secondary records can route traffic to anything from an Amazon S3 bucket that is configured as a website to a complex tree of records.

Weighted routing or failover routing or latency routing cannot be used to restrict the distribution of content to only the locations in which you have distribution rights. So all three options above are incorrect.

References:

[https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo)

[https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html)

[https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-awswaf.html)

---

## Question 2
AWS Web Application Firewall (AWS WAF) is a web application firewall service that lets you monitor web requests and protect your web applications from malicious requests. Use AWS WAF to block or allow requests based on conditions that you specify, such as the IP addresses. You can also use AWS WAF preconfigured protections to block common attacks like SQL injection or cross-site scripting.

**Configure AWS Web Application Firewall (AWS WAF) on the Application Load Balancer in a Amazon Virtual Private Cloud (Amazon VPC)**

You can use AWS WAF with your Application Load Balancer to allow or block requests based on the rules in a web access control list (web ACL). Geographic (Geo) Match Conditions in AWS WAF allows you to use AWS WAF to restrict application access based on the geographic location of your viewers. With geo match conditions you can choose the countries from which AWS WAF should allow access.

Geo match conditions are important for many customers. For example, legal and licensing requirements restrict some customers from delivering their applications outside certain countries. These customers can configure a whitelist that allows only viewers in those countries. Other customers need to prevent the downloading of their encrypted software by users in certain countries. These customers can configure a blacklist so that end-users from those countries are blocked from downloading their software.

### Incorrect options:

**Use Geo Restriction feature of Amazon CloudFront in a Amazon Virtual Private Cloud (Amazon VPC)** - Geo Restriction feature of Amazon CloudFront helps in restricting traffic based on the user's geographic location. But, CloudFront works from edge locations and doesn't belong to a VPC. Hence, this option itself is incorrect and given only as a distractor.

**Configure the security group on the Application Load Balancer**

**Configure the security group for the Amazon EC2 instances**

Security Groups cannot restrict access based on the user's geographic location.

References:

[https://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic-match/](https://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic-match/)

[https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/](https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/)

[https://aws.amazon.com/about-aws/whats-new/2016/12/AWS-WAF-now-available-on-Application-Load-Balancer/](https://aws.amazon.com/about-aws/whats-new/2016/12/AWS-WAF-now-available-on-Application-Load-Balancer/)

---

## Question 3
**Use server-side encryption with AWS Key Management Service keys (SSE-KMS) to encrypt the user data on Amazon S3**

AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Therefore SSE-KMS is the correct solution for this use-case.

Server Side Encryption in S3: ![](https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q22-i1.jpg)

![](https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q22-i1.jpg)

via - [https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html)

### Incorrect options:

**Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the user data on Amazon S3** - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. However this option does not provide the ability to audit trail the usage of the encryption keys.

**Use server-side encryption with customer-provided keys (SSE-C) to encrypt the user data on Amazon S3** - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects. However this option does not provide the ability to audit trail the usage of the encryption keys.

**Use client-side encryption with client provided keys and then upload the encrypted user data to Amazon S3** - Using client-side encryption is ruled out as the startup does not want to provide the encryption keys.

References:

[https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html)

[https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html)

[https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html)

----

## Question 4

**Create a strong password for the AWS account root user**

**Enable Multi Factor Authentication (MFA) for the AWS account root user account**

Here are some of the best practices while creating an AWS account root user:

1) Use a strong password to help protect account-level access to the AWS Management Console. 2) Never share your AWS account root user password or access keys with anyone. 3) If you do have an access key for your AWS account root user, delete it. If you must keep it, rotate (change) the access key regularly. You should not encrypt the access keys and save them on Amazon S3. 4) If you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to. 5) Enable AWS multi-factor authentication (MFA) on your AWS account root user account.

AWS Root Account Security Best Practices: ![](https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q31-i1.jpg)

![](https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q31-i1.jpg)

via - [https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html)

### Incorrect options:

**Encrypt the access keys and save them on Amazon S3** - AWS recommends that if you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to. Even an encrypted access key for the root user poses a significant security risk. Therefore, this option is incorrect.

**Create AWS account root user access keys and share those keys only with the business owner** - AWS recommends that if you don't already have an access key for your AWS account root user, don't create one unless you absolutely need to. Hence, this option is incorrect.

**Send an email to the business owner with details of the login username and password for the AWS root user. This will help the business owner to troubleshoot any login issues in future** - AWS recommends that you should never share your AWS account root user password or access keys with anyone. Sending an email with AWS account root user credentials creates a security risk as it can be misused by anyone reading the email. Hence, this option is incorrect.

Reference:

[https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#create-iam-users](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#create-iam-users)

----

## Question 5

**Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances**

Amazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. It also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately.

How Amazon GuardDuty works: ![](https://d1.awsstatic.com/product-marketing/Amazon%20GuardDuty/product-page-diagram-Amazon-GuardDuty_how-it-works.a4daf7e3aaf3532623a3797dd3af606a85fc2e7b.png)

![](https://d1.awsstatic.com/product-marketing/Amazon%20GuardDuty/product-page-diagram-Amazon-GuardDuty_how-it-works.a4daf7e3aaf3532623a3797dd3af606a85fc2e7b.png)

via - [https://aws.amazon.com/guardduty/](https://aws.amazon.com/guardduty/)

Amazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. Amazon Inspector assessments are offered to you as pre-defined rules packages mapped to common security best practices and vulnerability definitions.

### Incorrect options:

**Use Amazon GuardDuty to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances**

**Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon Inspector to check for vulnerabilities on Amazon EC2 instances**

**Use Amazon Inspector to monitor any malicious activity on data stored in Amazon S3. Use security assessments provided by Amazon GuardDuty to check for vulnerabilities on Amazon EC2 instances**

These three options contradict the explanation provided above, so these options are incorrect.

References:

[https://aws.amazon.com/guardduty/](https://aws.amazon.com/guardduty/)

[https://aws.amazon.com/inspector/](https://aws.amazon.com/inspector/)

---

## Question 6

**When you apply a retention period to an object version explicitly, you specify a `Retain Until Date` for the object version**

You can place a retention period on an object version either explicitly or through a bucket default setting. When you apply a retention period to an object version explicitly, you specify a `Retain Until Date` for the object version. Amazon S3 stores the Retain Until Date setting in the object version's metadata and protects the object version until the retention period expires.

**Different versions of a single object can have different retention modes and periods**

Like all other Object Lock settings, retention periods apply to individual object versions. Different versions of a single object can have different retention modes and periods.

For example, suppose that you have an object that is 15 days into a 30-day retention period, and you PUT an object into Amazon S3 with the same name and a 60-day retention period. In this case, your PUT succeeds, and Amazon S3 creates a new version of the object with a 60-day retention period. The older version maintains its original retention period and becomes deletable in 15 days.

### Incorrect options:

**You cannot place a retention period on an object version through a bucket default setting** - You can place a retention period on an object version either explicitly or through a bucket default setting.

**When you use bucket default settings, you specify a `Retain Until Date` for the object version** - When you use bucket default settings, you don't specify a Retain Until Date. Instead, you specify a duration, in either days or years, for which every object version placed in the bucket should be protected.

**The bucket default settings will override any explicit retention mode or period you request on an object version** - If your request to place an object version in a bucket contains an explicit retention mode and period, those settings override any bucket default settings for that object version.

Reference:

[https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html)

---

## Question 7
Amazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. It also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately.

**Disable the service in the general settings**

Disabling the service will delete all remaining data, including your findings and configurations before relinquishing the service permissions and resetting the service. So, this is the correct option for our use case.

### Incorrect options:

**Suspend the service in the general settings** - You can stop Amazon GuardDuty from analyzing your data sources at any time by choosing to suspend the service in the general settings. This will immediately stop the service from analyzing data, but does not delete your existing findings or configurations.

**De-register the service under services tab** - This is a made-up option, used only as a distractor.

**Raise a service request with Amazon to completely delete the data from all their backups** - There is no need to create a service request as you can delete the existing findings by disabling the service.

Reference:

[https://aws.amazon.com/guardduty/faqs/](https://aws.amazon.com/guardduty/faqs/)

---

## Question 8
**VPC Flow Logs, Domain Name System (DNS) logs, AWS CloudTrail events**

Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. With the cloud, the collection and aggregation of account and network activities is simplified, but it can be time-consuming for security teams to continuously analyze event log data for potential threats. With GuardDuty, you now have an intelligent and cost-effective option for continuous threat detection in AWS. The service uses machine learning, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats.

Amazon GuardDuty analyzes tens of billions of events across multiple AWS data sources, such as AWS CloudTrail events, Amazon VPC Flow Logs, and DNS logs.

With a few clicks in the AWS Management Console, GuardDuty can be enabled with no software or hardware to deploy or maintain. By integrating with Amazon EventBridge Events, GuardDuty alerts are actionable, easy to aggregate across multiple accounts, and straightforward to push into existing event management and workflow systems.

How Amazon GuardDuty works: ![](https://d1.awsstatic.com/product-marketing/Amazon%20GuardDuty/product-page-diagram-Amazon-GuardDuty_how-it-works.a4daf7e3aaf3532623a3797dd3af606a85fc2e7b.png)

![](https://d1.awsstatic.com/product-marketing/Amazon%20GuardDuty/product-page-diagram-Amazon-GuardDuty_how-it-works.a4daf7e3aaf3532623a3797dd3af606a85fc2e7b.png)

via - [https://aws.amazon.com/guardduty/](https://aws.amazon.com/guardduty/)

## Incorrect options:

**VPC Flow Logs, Amazon API Gateway logs, Amazon S3 access logs**

**Elastic Load Balancing logs, Domain Name System (DNS) logs, AWS CloudTrail events**

**Amazon CloudFront logs, Amazon API Gateway logs, AWS CloudTrail events**

These three options contradict the explanation provided above, so these options are incorrect.

Reference:

[https://aws.amazon.com/guardduty/](https://aws.amazon.com/guardduty/)

---

## Question 9
**Enable AWS Multi-Factor Authentication (AWS MFA) for privileged users**

As per the AWS best practices, it is better to enable Multi Factor Authentication (MFA) for privileged users via an MFA-enabled mobile device or hardware MFA token.

**Configure AWS CloudTrail to log all AWS Identity and Access Management (AWS IAM) actions**

AWS recommends to turn on AWS CloudTrail to log all IAM actions for monitoring and audit purposes.

### Incorrect options:

**Create a minimum number of accounts and share these account credentials among employees** - AWS recommends that user account credentials should not be shared between users. So, this option is incorrect.

**Grant maximum privileges to avoid assigning privileges again** - AWS recommends granting the least privileges required to complete a certain job and avoid giving excessive privileges which can be misused. So, this option is incorrect.

**Use user credentials to provide access specific permissions for Amazon EC2 instances** - It is highly recommended to use roles to grant access permissions for EC2 instances working on different AWS services. So, this option is incorrect.

References:

[https://aws.amazon.com/iam/](https://aws.amazon.com/iam/)

[https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html)

[https://aws.amazon.com/cloudtrail/faqs/](https://aws.amazon.com/cloudtrail/faqs/)

---

## Question 10
**Use an Amazon Aurora Global Database for the `games` table and use Amazon Aurora for the `users` and `games_played` tables**

Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. Aurora is not an in-memory database.

Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. Amazon Aurora Global Database is the correct choice for the given use-case.

For the given use-case, we, therefore, need to have two Aurora clusters, one for the global table (games table) and the other one for the local tables (users and games\_played tables).

### Incorrect options:

**Use an Amazon Aurora Global Database for the `games` table and use Amazon DynamoDB tables for the `users` and `games_played` tables**

**Use a Amazon DynamoDB global table for the `games` table and use Amazon Aurora for the `users` and `games_played` tables**

**Use a Amazon DynamoDB global table for the `games` table and use Amazon DynamoDB tables for the `users` and `games_played` tables**

Here, we want minimal application refactoring. Amazon DynamoDB and Amazon Aurora have a completely different APIs, due to Amazon Aurora being SQL and Amazon DynamoDB being NoSQL. So all three options are incorrect, as they have Amazon DynamoDB as one of the components.

Reference:

[https://aws.amazon.com/rds/aurora/faqs/](https://aws.amazon.com/rds/aurora/faqs/)

---

## Question 11
**Dynamic content, as determined at request time (cache-behavior configured to forward all headers)**

Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.

CloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content.

Dynamic content, as determined at request time (cache-behavior configured to forward all headers), does not flow through regional edge caches, but goes directly to the origin. So this option is correct.

**Proxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin**

Proxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin from the POPs and do not proxy through the regional edge caches. So this option is also correct.

How Amazon CloudFront Delivers Content: ![](https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q55-i1.jpg)

![](https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q55-i1.jpg)

via - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html

### Incorrect Options:

**E-commerce assets such as product photos**

**User-generated videos**

**Static content such as style sheets, JavaScript files**

The following type of content flows through the regional edge caches - user-generated content, such as video, photos, or artwork; e-commerce assets such as product photos and videos and static content such as style sheets, JavaScript files. Hence these three options are not correct.

Reference:

[https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html)

---

## Question 12
```json
    {
        "Action": [
            "s3:DeleteObject"
        ],
        "Resource": [
            "arn:aws:s3:::example-bucket/*"
        ],
        "Effect": "Allow"
    }
```

The main elements of a policy statement are:

1.  Effect: Specifies whether the statement will Allow or Deny an action (`Allow` is the effect defined here).
    
2.  Action: Describes a specific action or actions that will either be allowed or denied to run based on the Effect entered. API actions are unique to each service (`DeleteObject` is the action defined here).
    
3.  Resource: Specifies the resources—for example, an Amazon S3 bucket or objects—that the policy applies to in Amazon Resource Name (ARN) format ( `example-bucket/*` is the resource defined here).
    

This policy provides the necessary delete permissions on the resources of the Amazon S3 bucket to the group.

### Incorrect options:

```json
    {
        "Action": [
            "s3:*Object"
        ],
        "Resource": [
            "arn:aws:s3:::example-bucket/*"
        ],
        "Effect": "Allow"
    }
```
    

This policy is incorrect as the action value is invalid

```json
    {
        "Action": [
            "s3:*"
        ],
        "Resource": [
            "arn:aws:s3:::example-bucket/*"
        ],
        "Effect": "Allow"
    }
```
    

This policy is incorrect since it allows all actions on the resource, which violates the principle of least privilege, as required by the given use case.

```json
    {
        "Action": [
            "s3:DeleteObject"
        ],
        "Resource": [
            "arn:aws:s3:::example-bucket*"
        ],
        "Effect": "Allow"
    }
```
    

This is incorrect, as the resource name is incorrect. It should have a `/*` after the bucket name.

Reference:

[https://aws.amazon.com/blogs/security/techniques-for-writing-least-privilege-iam-policies/](https://aws.amazon.com/blogs/security/techniques-for-writing-least-privilege-iam-policies/)

---

## Question 13

**As the AWS KMS key was deleted a day ago, it must be in the 'pending deletion' status and hence you can just cancel the KMS key deletion and recover the key**

AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. AWS KMS is a secure and resilient service that uses hardware security modules that have been validated under FIPS 140-2.

Deleting an AWS KMS key in AWS Key Management Service (AWS KMS) is destructive and potentially dangerous. Therefore, AWS KMS enforces a waiting period. To delete a KMS key in AWS KMS you schedule key deletion. You can set the waiting period from a minimum of 7 days up to a maximum of 30 days. The default waiting period is 30 days. During the waiting period, the KMS key status and key state is Pending deletion. To recover the KMS key, you can cancel key deletion before the waiting period ends. After the waiting period ends you cannot cancel key deletion, and AWS KMS deletes the KMS key.

How Deleting AWS KMS keys Works: ![](https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q35-i1.jpg)

![](https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt1-q35-i1.jpg)

via - [https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html](https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html)

### Incorrect options:

**Contact AWS support to retrieve the AWS KMS key from their backup**

**The AWS KMS key can be recovered by the AWS root account user**

The AWS root account user cannot recover the AWS KMS key and the AWS support does not have access to KMS keys via any backups. Both these options just serve as distractors.

**The company should issue a notification on its web application informing the users about the loss of their data** - This option is not required as the data can be recovered via the cancel key deletion feature.

Reference:

[https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html](https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html)

---

## Question 14
**Create a new IAM role with the required permissions to access the resources in the production environment. The users can then assume this IAM role while accessing the resources from the production environment**

IAM roles allow you to delegate access to users or services that normally don't have access to your organization's AWS resources. IAM users or AWS services can assume a role to obtain temporary security credentials that can be used to make AWS API calls. Consequently, you don't have to share long-term credentials for access to a resource. Using IAM roles, it is possible to access cross-account resources.

### Incorrect options:

**Create new IAM user credentials for the production environment and share these credentials with the set of users from the development environment** - There is no need to create new IAM user credentials for the production environment, as you can use IAM roles to access cross-account resources.

**It is not possible to access cross-account resources** - You can use IAM roles to access cross-account resources.

**Both IAM roles and IAM users can be used interchangeably for cross-account access** - IAM roles and IAM users are separate IAM entities and should not be mixed. Only IAM roles can be used to access cross-account resources.

Reference:

[https://aws.amazon.com/iam/features/manage-roles/](https://aws.amazon.com/iam/features/manage-roles/)

---

## Question 15
**Use permissions boundary to control the maximum permissions employees can grant to the IAM principals**

A permissions boundary can be used to control the maximum permissions employees can grant to the IAM principals (that is, users and roles) that they create and manage. As the IAM administrator, you can define one or more permissions boundaries using managed policies and allow your employee to create a principal with this boundary. The employee can then attach a permissions policy to this principal. However, the effective permissions of the principal are the intersection of the permissions boundary and permissions policy. As a result, the new principal cannot exceed the boundary that you defined. Therefore, using the permissions boundary offers the right solution for this use-case.

Permission Boundary Example: ![](https://d2908q01vomqb2.cloudfront.net/22d200f8670dbdb3e253a90eee5098477c95c23d/2018/07/03/delegated-admin-02.png)

![](https://d2908q01vomqb2.cloudfront.net/22d200f8670dbdb3e253a90eee5098477c95c23d/2018/07/03/delegated-admin-02.png)

via - [https://aws.amazon.com/blogs/security/delegate-permission-management-to-developers-using-iam-permissions-boundaries/](https://aws.amazon.com/blogs/security/delegate-permission-management-to-developers-using-iam-permissions-boundaries/)

### Incorrect options:

**Remove full database access for all IAM users in the organization** - It is not practical to remove full access for all IAM users in the organization because a select set of users need this access for database administration. So this option is not correct.

**The CTO should review the permissions for each new developer's IAM user so that such incidents don't recur** - Likewise the CTO is not expected to review the permissions for each new developer's IAM user, as this is best done via an automated procedure. This option has been added as a distractor.

**Only root user should have full database access in the organization** - As a best practice, the root user should not access the AWS account to carry out any administrative procedures. So this option is not correct.

Reference:

[https://aws.amazon.com/blogs/security/delegate-permission-management-to-developers-using-iam-permissions-boundaries/](https://aws.amazon.com/blogs/security/delegate-permission-management-to-developers-using-iam-permissions-boundaries/)